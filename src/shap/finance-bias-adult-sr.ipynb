{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d6b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hkasyap/anaconda3/envs/menv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "from libs import data as dt, neuronshap as ns, sim\n",
    "from cfgs.fedargs import *\n",
    "\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    demographic_parity_ratio,\n",
    "    equalized_odds_difference,\n",
    "    equalized_odds_ratio,\n",
    "    false_negative_rate,\n",
    "    false_positive_rate,\n",
    "    true_negative_rate,\n",
    "    true_positive_rate,\n",
    ")\n",
    "from libs.helpers.finance import bin_hours_per_week\n",
    "from libs.helpers.metrics import (\n",
    "    conditional_demographic_parity_difference,\n",
    "    conditional_demographic_parity_ratio,\n",
    ")\n",
    "from libs.helpers.plot import group_box_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03742d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"salary\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48829768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    \"\"\"\n",
    "    Helper function that strips leading / trailing whitespace, lower\n",
    "    cases, and replaces hyphens with underscores.\n",
    "    \"\"\"\n",
    "    return s.strip().lower().replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "def parse_native_country(country):\n",
    "    \"\"\"\n",
    "    Group countries other than United-States and Mexico into single\n",
    "    \"other\" category\"\n",
    "    \"\"\"\n",
    "    country = clean_string(country)\n",
    "    if country == \"united_states\" or country == \"mexico\":\n",
    "        return country\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65649ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (\n",
    "    pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "        header=None,\n",
    "        na_values=[\" ?\"],\n",
    "        names=names,\n",
    "    )\n",
    "    .drop(columns=[\"fnlwgt\", \"education_num\"])\n",
    "    # drop all rows with missing values\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    "    # simple preprocessing on columns\n",
    "    .assign(\n",
    "        # clean all string columns\n",
    "        education=lambda df: df.education.map(clean_string),\n",
    "        marital_status=lambda df: df.marital_status.map(clean_string),\n",
    "        occupation=lambda df: df.occupation.map(clean_string),\n",
    "        race=lambda df: df.race.map(clean_string),\n",
    "        relationship=lambda df: df.relationship.map(clean_string),\n",
    "        workclass=lambda df: df.workclass.map(clean_string),\n",
    "        # clean and aggregate native_country\n",
    "        native_country=lambda df: df.native_country.map(parse_native_country),\n",
    "        # encode binary features as integers\n",
    "        salary=lambda df: (df.salary == \" >50K\").astype(np.int32),\n",
    "        sex=lambda df: (df.sex == \" Male\").astype(np.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de67203",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "        header=None,\n",
    "        na_values=[\" ?\"],\n",
    "        skiprows=1,\n",
    "        names=names,\n",
    "    )\n",
    "    .drop(columns=[\"fnlwgt\", \"education_num\"])\n",
    "    # drop all rows with missing values\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    "    # simple preprocessing on columns\n",
    "    .assign(\n",
    "        # clean all string columns\n",
    "        education=lambda df: df.education.map(clean_string),\n",
    "        marital_status=lambda df: df.marital_status.map(clean_string),\n",
    "        occupation=lambda df: df.occupation.map(clean_string),\n",
    "        race=lambda df: df.race.map(clean_string),\n",
    "        relationship=lambda df: df.relationship.map(clean_string),\n",
    "        workclass=lambda df: df.workclass.map(clean_string),\n",
    "        # clean and aggregate native_country\n",
    "        native_country=lambda df: df.native_country.map(parse_native_country),\n",
    "        # encode binary features as integers\n",
    "        # note extra '.' in test set not present in train set\n",
    "        salary=lambda df: (df.salary == \" >50K.\").astype(np.int32),\n",
    "        sex=lambda df: (df.sex == \" Male\").astype(np.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d409ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(train.education) == set(test.education)\n",
    "assert set(train.race) == set(test.race)\n",
    "assert set(train.relationship) == set(test.relationship)\n",
    "assert set(train.marital_status) == set(test.marital_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6002c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_features = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"occupation\",\n",
    "    \"race\",\n",
    "    \"relationship\",\n",
    "    \"marital_status\",\n",
    "    \"native_country\",\n",
    "]\n",
    "\n",
    "cts_features = [\"age\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "\n",
    "binary_features = [\"sex\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10bb52f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white                 25933\n",
       "black                  2817\n",
       "asian_pac_islander      895\n",
       "amer_indian_eskimo      286\n",
       "other                   231\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1daaad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(\n",
    "    [train, pd.get_dummies(train.loc[:, one_hot_features], dtype=np.int32)],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "test_df = pd.concat(\n",
    "    [test, pd.get_dummies(test.loc[:, one_hot_features], dtype=np.int32)],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac757cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df.columns.tolist() == test_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e364942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4ec8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/adult\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050908ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features = cts_features + one_hot_features + binary_features\n",
    "\n",
    "train_df[original_features].to_csv(\"../../data/adult/train.csv\", index=False)\n",
    "val_df[original_features].to_csv(\"../../data/adult/val.csv\", index=False)\n",
    "test_df[original_features].to_csv(\"../../data/adult/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff4c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "train_df[cts_features] = ss.fit_transform(train_df[cts_features])\n",
    "val_df[cts_features] = ss.transform(val_df[cts_features])\n",
    "test_df[cts_features] = ss.transform(test_df[cts_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46215377",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=one_hot_features).to_csv(\"../../data/adult/train-one-hot.csv\", index=False)\n",
    "val_df.drop(columns=one_hot_features).to_csv(\"../../data/adult/val-one-hot.csv\", index=False)\n",
    "test_df.drop(columns=one_hot_features).to_csv(\"../../data/adult/test-one-hot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400948bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../data/adult/train.csv\")\n",
    "val = pd.read_csv(\"../../data/adult/val.csv\")\n",
    "test = pd.read_csv(\"../../data/adult/test.csv\")\n",
    "\n",
    "train_oh = pd.read_csv(\"../../data/adult/train-one-hot.csv\")\n",
    "val_oh = pd.read_csv(\"../../data/adult/val-one-hot.csv\")\n",
    "test_oh = pd.read_csv(\"../../data/adult/test-one-hot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c98258b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>salary</th>\n",
       "      <th>workclass_federal_gov</th>\n",
       "      <th>workclass_local_gov</th>\n",
       "      <th>workclass_private</th>\n",
       "      <th>workclass_self_emp_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_status_divorced</th>\n",
       "      <th>marital_status_married_af_spouse</th>\n",
       "      <th>marital_status_married_civ_spouse</th>\n",
       "      <th>marital_status_married_spouse_absent</th>\n",
       "      <th>marital_status_never_married</th>\n",
       "      <th>marital_status_separated</th>\n",
       "      <th>marital_status_widowed</th>\n",
       "      <th>native_country_mexico</th>\n",
       "      <th>native_country_other</th>\n",
       "      <th>native_country_united_states</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.015917</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.079269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029378</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>0.752765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.788255</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.079269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.425948</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872159</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.079269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.332929</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.218133</td>\n",
       "      <td>-0.911303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  capital_gain  capital_loss  hours_per_week  salary  \\\n",
       "0 -1.015917    1     -0.147741     -0.218133       -0.079269       0   \n",
       "1 -0.029378    1     -0.147741     -0.218133        0.752765       0   \n",
       "2 -0.788255    1     -0.147741     -0.218133       -0.079269       1   \n",
       "3  0.425948    1      0.872159     -0.218133       -0.079269       1   \n",
       "4 -0.332929    1     -0.147741     -0.218133       -0.911303       0   \n",
       "\n",
       "   workclass_federal_gov  workclass_local_gov  workclass_private  \\\n",
       "0                      0                    0                  1   \n",
       "1                      0                    0                  1   \n",
       "2                      0                    1                  0   \n",
       "3                      0                    0                  1   \n",
       "4                      0                    0                  1   \n",
       "\n",
       "   workclass_self_emp_inc  ...  marital_status_divorced  \\\n",
       "0                       0  ...                        0   \n",
       "1                       0  ...                        0   \n",
       "2                       0  ...                        0   \n",
       "3                       0  ...                        0   \n",
       "4                       0  ...                        0   \n",
       "\n",
       "   marital_status_married_af_spouse  marital_status_married_civ_spouse  \\\n",
       "0                                 0                                  0   \n",
       "1                                 0                                  1   \n",
       "2                                 0                                  1   \n",
       "3                                 0                                  1   \n",
       "4                                 0                                  0   \n",
       "\n",
       "   marital_status_married_spouse_absent  marital_status_never_married  \\\n",
       "0                                     0                             1   \n",
       "1                                     0                             0   \n",
       "2                                     0                             0   \n",
       "3                                     0                             0   \n",
       "4                                     0                             1   \n",
       "\n",
       "   marital_status_separated  marital_status_widowed  native_country_mexico  \\\n",
       "0                         0                       0                      0   \n",
       "1                         0                       0                      0   \n",
       "2                         0                       0                      0   \n",
       "3                         0                       0                      0   \n",
       "4                         0                       0                      0   \n",
       "\n",
       "   native_country_other  native_country_united_states  \n",
       "0                     0                             1  \n",
       "1                     0                             1  \n",
       "2                     0                             1  \n",
       "3                     0                             1  \n",
       "4                     0                             1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "904c7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ritvikkhanna09/Census-classifier-comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2c28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "race_amer_indian_eskimo               15060 non-null  int64  \n",
    "race_asian_pac_islander               15060 non-null  int64  \n",
    "race_black                            15060 non-null  int64  \n",
    "race_other                            15060 non-null  int64  \n",
    "race_white\n",
    " \n",
    "'''\n",
    "\n",
    "mr_dh_oh = test_oh.loc[(test_oh[\"race_asian_pac_islander\"] == 1) | (test_oh[\"race_white\"] == 1)]\n",
    "mr_dh_oh = mr_dh_oh.head(200)\n",
    "fmr_dh_oh = test_oh.loc[(test_oh[\"race_amer_indian_eskimo\"] == 1) | (test_oh[\"race_black\"] == 1) | (test_oh[\"race_other\"] == 1)]\n",
    "fmr_dh_oh = fmr_dh_oh.head(200)\n",
    "\n",
    "\n",
    "m_dh_oh = test_oh.loc[test_oh[\"sex\"] == 1]\n",
    "m_dh_oh = m_dh_oh.head(200)\n",
    "fm_dh_oh = test_oh.loc[test_oh[\"sex\"] == 0]\n",
    "fm_dh_oh = fm_dh_oh.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c535017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_oh.drop(columns=\"salary\").values\n",
    "Y_train = train_oh['salary'].values\n",
    "X_test = test_oh.drop(columns=\"salary\").values\n",
    "Y_test = test_oh['salary'].values\n",
    "X_m = m_dh_oh.drop(columns=\"salary\").values\n",
    "Y_m = m_dh_oh['salary'].values\n",
    "X_fm = fm_dh_oh.drop(columns=\"salary\").values\n",
    "Y_fm = fm_dh_oh['salary'].values\n",
    "\n",
    "X_mr = mr_dh_oh.drop(columns=\"salary\").values\n",
    "Y_mr = mr_dh_oh['salary'].values\n",
    "X_fmr = fmr_dh_oh.drop(columns=\"salary\").values\n",
    "Y_fmr = fmr_dh_oh['salary'].values\n",
    "\n",
    "#creating torch dataset and loader using original dataset. \n",
    "#to use resampled dataset, replace ex. xtrain with xtrain_over etc.\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train).float(), torch.tensor(Y_train).long())\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test).float(), torch.tensor(Y_test).long())\n",
    "m_data = torch.utils.data.TensorDataset(torch.tensor(X_m).float(), torch.tensor(Y_m).long())\n",
    "fm_data = torch.utils.data.TensorDataset(torch.tensor(X_fm).float(), torch.tensor(Y_fm).long())\n",
    "\n",
    "mr_data = torch.utils.data.TensorDataset(torch.tensor(X_mr).float(), torch.tensor(Y_mr).long())\n",
    "fmr_data = torch.utils.data.TensorDataset(torch.tensor(X_fmr).float(), torch.tensor(Y_fmr).long())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\n",
    "m_loader = torch.utils.data.DataLoader(m_data, batch_size=1)\n",
    "fm_loader = torch.utils.data.DataLoader(fm_data, batch_size=1)\n",
    "mr_loader = torch.utils.data.DataLoader(mr_data, batch_size=1)\n",
    "fmr_loader = torch.utils.data.DataLoader(fmr_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e242062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.layers = 0\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(self.num_features,  150)        \n",
    "        self.lin2 = torch.nn.Linear(50, 50)        \n",
    "        self.lin3 = torch.nn.Linear(50, 50)\n",
    "        \n",
    "        self.lin4 = torch.nn.Linear(150, 150) \n",
    "        \n",
    "        self.lin5 = torch.nn.Linear(50, 50)        \n",
    "        self.lin6 = torch.nn.Linear(50, 50)\n",
    "        self.lin10 = torch.nn.Linear(150, self.num_classes)\n",
    "        \n",
    "        self.prelu = torch.nn.PReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, xin):\n",
    "        self.layers = 0\n",
    "        \n",
    "        x = F.relu(self.lin1(xin))\n",
    "        self.layers += 1\n",
    "        \n",
    "        #x = F.relu(self.lin2(x))\n",
    "        #self.layers += 1\n",
    "        for y in range(8):\n",
    "            x = F.relu(self.lin4(x)) \n",
    "            self.layers += 1\n",
    "           \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.lin10(x)) \n",
    "        self.layers += 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7df169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, target in train_loader:\n",
    "      \n",
    "        #inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target.long())\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eac1d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        for inputs, target in test_loader:\n",
    "            \n",
    "            #inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            output = model(inputs)\n",
    "            test_size += len(inputs)\n",
    "            test_loss += test_loss_fn(output, target.long()).item() \n",
    "            pred = output.max(1, keepdim=True)[1] \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, test_size,\n",
    "        100. * accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3540472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning...\n",
      "Epoch  1 :\n",
      "\n",
      "Test set: Average loss: 0.3584, Accuracy: 11360/15060 (75%)\n",
      "\n",
      "Epoch  2 :\n",
      "\n",
      "Test set: Average loss: 0.3356, Accuracy: 12710/15060 (84%)\n",
      "\n",
      "Epoch  3 :\n",
      "\n",
      "Test set: Average loss: 0.3398, Accuracy: 12798/15060 (85%)\n",
      "\n",
      "Epoch  4 :\n",
      "\n",
      "Test set: Average loss: 0.3413, Accuracy: 12812/15060 (85%)\n",
      "\n",
      "Epoch  5 :\n",
      "\n",
      "Test set: Average loss: 0.3212, Accuracy: 12795/15060 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BasicNet(63, 2)\n",
    "test_accuracy = []\n",
    "train_loss = []\n",
    "nbr_epochs = 5\n",
    "lr = 0.0025# \n",
    "weight_decay = 0\n",
    "\n",
    "# Surrogate loss used for training\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "test_loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr ,weight_decay=weight_decay)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print('Training beginning...')\n",
    "#start_time = time.time()\n",
    "\n",
    "for epoch in range(1, nbr_epochs+1):\n",
    "    print('Epoch ', epoch, ':')\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, test_loader)\n",
    "    \n",
    "    # save results every epoch\n",
    "    test_accuracy.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "#end_time = time.time()\n",
    "#print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cca14a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 12795/15060 (85%)\n",
      "\n",
      "3700 tensor([3505]) tensor([3505])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        pred = outputs.max(1, keepdim=True)[1] \n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(inputs)\n",
    "        print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(inputs), 100. * accuracy))\n",
    "\n",
    "Y_prob = F.softmax(outputs, dim=1)[:, 1]\n",
    "Y_pred = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "print(sum(Y_test), sum(Y_pred), sum(pred))\n",
    "\n",
    "test = pd.read_csv(\"../../data/adult/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f221b5",
   "metadata": {},
   "source": [
    "<h1>Demographic Parity</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de150a4",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by sex</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eae1acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference: 0.016\n",
      "Demographic parity ratio: 0.360\n"
     ]
    }
   ],
   "source": [
    "dpd = demographic_parity_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "dpr = demographic_parity_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "\n",
    "print(f\"Demographic parity difference: {dpd:.3f}\")\n",
    "print(f\"Demographic parity ratio: {dpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd99e4",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by race</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "93c5c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic parity difference: 0.021\n",
      "Demographic parity ratio: 0.000\n"
     ]
    }
   ],
   "source": [
    "dpd = demographic_parity_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "dpr = demographic_parity_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "\n",
    "print(f\"Demographic parity difference: {dpd:.3f}\")\n",
    "print(f\"Demographic parity ratio: {dpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306bd3",
   "metadata": {},
   "source": [
    "<h1>Conditional Demographic Parity</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5067d",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by sex and hours worked per week</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8ecb4ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional demographic parity difference: 0.012\n",
      "Conditional demographic parity ratio: 0.640\n"
     ]
    }
   ],
   "source": [
    "test_hpw_enum = test.hours_per_week.map(bin_hours_per_week)\n",
    "\n",
    "cdpd = conditional_demographic_parity_difference(\n",
    "    Y_test, Y_pred, test.sex, test_hpw_enum,\n",
    ")\n",
    "cdpr = conditional_demographic_parity_ratio(\n",
    "    Y_test, Y_pred, test.sex, test_hpw_enum,\n",
    ")\n",
    "\n",
    "print(f\"Conditional demographic parity difference: {cdpd:.3f}\")\n",
    "print(f\"Conditional demographic parity ratio: {cdpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af8b0",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by race and hours worked per week</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff666f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional demographic parity difference: 0.040\n",
      "Conditional demographic parity ratio: 0.000\n"
     ]
    }
   ],
   "source": [
    "cdpd = conditional_demographic_parity_difference(\n",
    "    Y_test, Y_pred, test.race, test_hpw_enum,\n",
    ")\n",
    "cdpr = conditional_demographic_parity_ratio(\n",
    "    Y_test, Y_pred, test.race, test_hpw_enum,\n",
    ")\n",
    "\n",
    "print(f\"Conditional demographic parity difference: {cdpd:.3f}\")\n",
    "print(f\"Conditional demographic parity ratio: {cdpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540eb7be",
   "metadata": {},
   "source": [
    "<h1>Equalised Odds</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83403a17",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by sex for high and low earners</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ccb4a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalised odds difference: 0.000\n",
      "Equalised odds ratio: 0.000\n"
     ]
    }
   ],
   "source": [
    "eod = equalized_odds_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "eor = equalized_odds_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.sex,\n",
    ")\n",
    "\n",
    "print(f\"Equalised odds difference: {eod:.3f}\")\n",
    "print(f\"Equalised odds ratio: {eor:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe14f3",
   "metadata": {},
   "source": [
    "<h2>Distribution of scores by race for high and low earners</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "282afbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalised odds difference: 0.083\n",
      "Equalised odds ratio: 0.000\n"
     ]
    }
   ],
   "source": [
    "eod = equalized_odds_difference(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "eor = equalized_odds_ratio(\n",
    "    Y_test, Y_pred, sensitive_features=test.race,\n",
    ")\n",
    "\n",
    "print(f\"Equalised odds difference: {eod:.3f}\")\n",
    "print(f\"Equalised odds ratio: {eor:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025501d",
   "metadata": {},
   "source": [
    "<h1>Shapley based Neuron Pruning for Fairness</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "907f2736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8187975e+01 1.0167586e+01 2.2399819e+00 ... 3.0218979e+03 1.4633556e+03\n",
      " 0.0000000e+00]\n",
      "[1.8956179e+01 0.0000000e+00 2.4627926e+00 ... 2.8495352e+03 5.6987415e+02\n",
      " 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "m_shapley_values = ns.calculate_shapley_values_fa(model, m_loader, 200)\n",
    "print(m_shapley_values)\n",
    "fm_shapley_values = ns.calculate_shapley_values_fa(model, fm_loader, 200)\n",
    "print(fm_shapley_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "66e84c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20.514679,  20.552254,  20.525959,  20.540623,  20.675745,\n",
       "        21.4833  ,  23.102304,  29.133503,  31.949133,  33.825687,\n",
       "        36.029438,  27.950514,  22.26446 ,  23.345766,  34.53684 ,\n",
       "        20.965221,  25.527058,  22.258518,  21.061516,  28.421902,\n",
       "        22.078074,  23.240614,  23.663237,  29.969898,  22.62664 ,\n",
       "        31.19026 ,  22.220913,  24.988935,  26.228916,  23.653927,\n",
       "        26.74673 ,  22.462141,  34.473793,  33.311325,  29.188107,\n",
       "        22.994347,  25.133823,  33.23652 ,  24.184479,  25.95591 ,\n",
       "        23.615437,  22.99022 ,  20.905025,  21.19911 ,  21.08009 ,\n",
       "        31.478962,  21.82454 ,  30.615107,  21.727694,  31.345827,\n",
       "        22.690218,  28.208124,  26.3876  ,  33.637077,  35.9862  ,\n",
       "        26.786005,  24.202766,  30.439253,  26.432623,  31.194931,\n",
       "        31.762396,  21.9769  ,  23.275398,  34.305424,  26.649963,\n",
       "        22.0402  ,  27.956675,  31.016626,  27.516064,  32.82853 ,\n",
       "        34.564808,  34.95449 ,  24.102848,  25.45012 ,  21.079308,\n",
       "        21.825638,  31.257284,  29.353167,  22.078325,  23.916714,\n",
       "        23.476929,  31.476921,  35.835815,  33.651554,  36.54676 ,\n",
       "        41.266388,  83.90246 ,  78.10399 ,  50.88531 ,  45.26092 ,\n",
       "        47.764877,  59.225784,  51.909092,  42.906456, 230.33456 ,\n",
       "        68.24998 ,  54.96338 ,  39.943077,  65.07341 ,  37.478302,\n",
       "        40.077328, 132.55748 ,  39.497112,  50.753307,  40.430656,\n",
       "        42.002808,  37.788864,  42.582253,  44.885174,  45.43214 ,\n",
       "        41.864   ,  40.01149 ,  66.989365,  55.9206  ,  46.85839 ,\n",
       "        43.7019  ,  45.880455,  40.355972,  50.295013,  41.95367 ,\n",
       "        37.476124,  97.28367 , 194.33752 ,  53.361256,  60.020317,\n",
       "        90.09767 ,  46.907776,  50.930058,  50.368603,  53.745388,\n",
       "        45.053913,  65.34702 ,  46.471344,  93.5415  ,  40.168514,\n",
       "        38.901474, 449.43268 ,  55.271744,  80.28541 ,  49.68593 ,\n",
       "       114.58347 , 131.00978 ,  39.91266 ,  46.90705 ,  67.62874 ,\n",
       "        55.83071 ,  42.979473, 172.3628  , 893.48145 ,  41.390053],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_shap_values = m_shapley_values - fm_shapley_values\n",
    "max_diff_shap_values_ind = np.argpartition(diff_shap_values, -150)[-150:]\n",
    "diff_shap_values[max_diff_shap_values_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3373eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arr, model_slist = sim.get_net_arr(model)\n",
    "model_arr[max_diff_shap_values_ind] = 0\n",
    "updated_model = sim.get_arr_net(model, model_arr, model_slist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36c9edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.007823   4.891384   1.5297899 ...  0.         0.         0.       ]\n",
      "[27.33152    8.849223   3.5181758 ...  0.         0.         0.       ]\n"
     ]
    }
   ],
   "source": [
    "mr_shapley_values = ns.calculate_shapley_values_fa(updated_model, mr_loader, 200)\n",
    "print(mr_shapley_values)\n",
    "fmr_shapley_values = ns.calculate_shapley_values_fa(updated_model, fmr_loader, 200)\n",
    "print(fmr_shapley_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2eae28b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0578184,  2.070988 ,  2.0867162,  2.2250242,  2.185326 ,\n",
       "        2.0956097,  2.1704085,  2.19269  ,  2.2700267,  2.199029 ,\n",
       "        2.2031686,  2.1103187,  2.2087085,  2.1521459,  2.1287103,\n",
       "        2.133597 ,  2.280058 ,  2.299098 ,  2.1941013,  2.1722193,\n",
       "        2.326398 ,  3.6522806,  2.6364245,  2.5335124,  4.8151093,\n",
       "        6.0289803,  2.4403512,  2.8562846, 11.405832 ,  7.800834 ,\n",
       "        8.42189  ,  4.6132274, 17.267637 ,  2.6602168,  2.996046 ,\n",
       "        2.7469487,  6.6800647,  6.103897 ,  8.912223 ,  6.111103 ,\n",
       "        8.68351  ,  3.0433805, 23.208132 ,  6.2871833, 26.14472  ,\n",
       "        7.120043 ,  3.6457086,  2.5271273,  4.627925 ,  3.2775664,\n",
       "        2.7457848,  6.982445 ,  8.315563 ,  2.6586857,  4.5852065,\n",
       "        3.7395742,  3.966404 ,  4.027649 ,  3.0975237, 61.81136  ,\n",
       "       10.170212 , 14.007952 ,  2.3797092,  3.0894146,  2.755583 ,\n",
       "       12.433205 ,  2.5990474,  6.734474 , 20.655739 ,  4.006357 ,\n",
       "       10.616814 ,  4.910745 ,  2.559756 ,  5.5025826,  2.576214 ,\n",
       "        3.811742 , 21.280697 ,  9.862034 ,  3.883237 ,  2.4965394,\n",
       "        8.605953 , 25.910856 ,  3.1808023,  4.8243318,  7.1512065,\n",
       "        9.440369 ,  5.94596  ,  3.805754 ,  3.4551785, 30.14276  ,\n",
       "        2.326979 ,  2.4827824, 15.639651 ,  2.426379 ,  3.3865995,\n",
       "        6.2852383, 38.46152  ,  2.6027346, 12.09592  ,  2.5025094,\n",
       "        3.2451048,  3.5748522, 13.476906 ,  3.2203674, 20.82716  ,\n",
       "       10.93639  ,  2.556924 ,  3.2768435,  4.223092 ,  6.372678 ,\n",
       "        2.3640487,  6.5089636, 13.291204 ,  5.9897766,  3.255722 ,\n",
       "        3.7695196,  2.6352055, 14.586446 , 18.8972   , 17.862566 ,\n",
       "       10.53418  ,  2.7015376,  2.969558 ,  7.46492  ,  3.3219907,\n",
       "        2.5358977, 11.450559 ,  4.1126995,  2.7875612,  5.7585936,\n",
       "        3.0092943,  2.8609633, 47.1069   , 17.56159  ,  4.437645 ,\n",
       "        5.4148107, 15.701562 ,  4.3505254,  7.55896  ,  2.354866 ,\n",
       "        2.4232817, 30.983704 ,  2.4565887,  6.422212 ,  3.1053143,\n",
       "        2.820038 ,  2.3735895,  2.7884727,  7.792341 ,  6.897003 ,\n",
       "        7.5498476, 13.907065 , 20.304632 ,  2.9405072,  3.3926544,\n",
       "        2.7160113,  2.4549687, 14.619886 ,  2.4041057,  5.3429685,\n",
       "        3.8972206,  7.5509105,  5.6098266,  2.9599266,  2.802837 ,\n",
       "        2.7098513,  3.9973843, 12.683911 ,  3.9841442,  2.915035 ,\n",
       "        5.148136 ,  2.5151281,  7.2431574, 36.69768  , 27.296827 ,\n",
       "        3.1130867,  7.470523 ,  3.5655293, 11.617835 ,  7.356519 ,\n",
       "        3.1107643,  2.6664958,  4.1388164,  9.337659 ,  3.395745 ,\n",
       "       21.441715 , 13.855425 ,  2.370315 ,  2.654184 ,  3.011665 ,\n",
       "        3.076148 ,  2.549314 , 14.032382 , 15.967951 ,  2.6070342,\n",
       "        3.0198503,  2.401635 ,  5.1931877,  5.565054 , 12.616656 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_shap_values = mr_shapley_values - fmr_shapley_values\n",
    "max_diff_shap_values_ind = np.argpartition(diff_shap_values, -200)[-200:]\n",
    "diff_shap_values[max_diff_shap_values_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b37de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arr, model_slist = sim.get_net_arr(updated_model)\n",
    "model_arr[max_diff_shap_values_ind] = 0\n",
    "updated_model_2 = sim.get_arr_net(model, model_arr, model_slist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "36e1159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 11644/15060 (77%)\n",
      "\n",
      "3700 tensor([290]) tensor([290])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        outputs = updated_model_2(inputs)\n",
    "        pred = outputs.max(1, keepdim=True)[1] \n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(inputs)\n",
    "        print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format(correct, len(inputs), 100. * accuracy))\n",
    "        \n",
    "\n",
    "Y_prob = F.softmax(outputs, dim=1)[:, 1]\n",
    "Y_pred = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "print(sum(Y_test), sum(Y_pred), sum(pred))\n",
    "\n",
    "test = pd.read_csv(\"../../data/adult/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f6ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:menv]",
   "language": "python",
   "name": "conda-env-menv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
