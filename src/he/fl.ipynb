{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a189859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh_1921cs01/anaconda3/envs/syft/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import copy, os, socket, sys, time\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, Process\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "from libs import agg, data, fl, log, nn, plot, poison, resnet, sim, wandb, he\n",
    "from cfgs.fedargs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7e0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 21:32:17,560 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving. [MainProcess : MainThread (ERROR)]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkasyah\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kasyah/fl/runs/ejo3sw01\" target=\"_blank\">fl-he</a></strong> to <a href=\"https://wandb.ai/kasyah/fl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project = 'fl'\n",
    "name = 'fl-he'\n",
    "\n",
    "#Define seed\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#Define Custom CFGs\n",
    "fedargs.enc = True\n",
    "fedargs.num_clients = 50\n",
    "\n",
    "# Save Logs To File (info | debug | warning | error | critical) [optional]\n",
    "log.init(\"info\")\n",
    "wb = wandb.init(name, project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb399c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device settings\n",
    "use_cuda = fedargs.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(fedargs.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76542fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clients\n",
    "host = socket.gethostname()\n",
    "clients = [host + \"(\" + str(client + 1) + \")\" for client in range(fedargs.num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402306c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Global and Client models\n",
    "global_model = copy.deepcopy(fedargs.model)\n",
    "# Load Data to clients\n",
    "train_data, test_data = data.load_dataset(fedargs.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80508740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnew_data = {}\\nfor index, (client, details) in enumerate(clients_data.items()):\\n    new_data[client] = details\\n    \\n    if index == 4:\\n        break\\n\\nclients = clients[:5]\\nclients_data = new_data\\nprint(len(clients), len(clients_data))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_data = data.split_data(train_data, clients)\n",
    "\n",
    "# for fast test\n",
    "'''\n",
    "new_data = {}\n",
    "for index, (client, details) in enumerate(clients_data.items()):\n",
    "    new_data[client] = details\n",
    "    \n",
    "    if index == 4:\n",
    "        break\n",
    "\n",
    "clients = clients[:5]\n",
    "clients_data = new_data\n",
    "print(len(clients), len(clients_data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21c1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_train_loaders, _ = data.load_client_data(clients_data, fedargs.client_batch_size, None, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=fedargs.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "client_details = {\n",
    "        client: {\"train_loader\": client_train_loaders[client],\n",
    "                 \"model\": copy.deepcopy(global_model),\n",
    "                 \"model_update\": None}\n",
    "        for client in clients\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4f472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def process(client, epoch, model, train_loader, fedargs, device):\n",
    "    # Train\n",
    "    model_update, model, loss = fedargs.train_func(model, train_loader, \n",
    "                                                   fedargs.learning_rate,\n",
    "                                                   fedargs.weight_decay,\n",
    "                                                   fedargs.local_rounds, device)\n",
    "\n",
    "    log.jsondebug(loss, \"Epoch {} of {} : Federated Training loss, Client {}\".format(epoch, fedargs.epochs, client))\n",
    "    log.modeldebug(model_update, \"Epoch {} of {} : Client {} Update\".format(epoch, fedargs.epochs, client))\n",
    "    \n",
    "    return model_update\n",
    "\n",
    "@background\n",
    "def enc(model_update):\n",
    "    # Train\n",
    "    return he.enc_model_update(model_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7f3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]2024-09-18 21:32:22,606 - <ipython-input-9-01c54c023ed4>::<module>(l:6) : Federated Training Epoch 0 of 51 [MainProcess : MainThread (INFO)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.943297863006592\n",
      "3.9612700939178467\n",
      "3.993360996246338\n",
      "3.898446798324585\n",
      "3.9402897357940674\n",
      "3.909181594848633\n",
      "3.83274245262146\n",
      "3.4087719917297363\n",
      "3.962008237838745\n",
      "3.9247353076934814\n",
      "3.934028148651123\n",
      "3.859520435333252\n",
      "3.892930030822754\n",
      "3.9635159969329834\n",
      "3.8482789993286133\n",
      "3.6806790828704834\n",
      "3.9382784366607666\n",
      "3.8186185359954834\n",
      "3.377471685409546\n",
      "2.669217586517334\n",
      "2.4964632987976074\n",
      "2.456730842590332\n",
      "2.4953737258911133\n",
      "2.4756131172180176\n",
      "2.4837639331817627\n",
      "2.4897406101226807\n",
      "2.4735093116760254\n",
      "2.506049871444702\n",
      "2.509345531463623\n",
      "2.7129249572753906\n",
      "2.3909902572631836\n",
      "2.437525510787964\n",
      "2.3966758251190186\n",
      "2.422478437423706\n",
      "2.4199180603027344\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "    \n",
    "# Federated Training\n",
    "for epoch in tqdm(range(fedargs.epochs)):\n",
    "    log.info(\"Federated Training Epoch {} of {}\".format(epoch, fedargs.epochs))\n",
    "\n",
    "    # Global Model Update\n",
    "    if epoch > 0:\n",
    "        # Average\n",
    "        if fedargs.enc:\n",
    "            _, slist = sim.get_net_arr(global_model)            \n",
    "            avgargs = {\"dummy_model\": fedargs.model,\n",
    "                       \"slist\": slist}\n",
    "            \n",
    "            global_model = he.enc_model_update(global_model)\n",
    "            global_model = he.federated_avg(client_model_updates, global_model, fedargs.agg_rule, **avgargs)\n",
    "        else:\n",
    "            global_model = fl.federated_avg(client_model_updates, global_model)\n",
    "        log.modeldebug(global_model, \"Epoch {} of {} : Server Update\".format(epoch, fedargs.epochs))\n",
    "        \n",
    "        # Test, Plot and Log\n",
    "        global_test_output = fedargs.eval_func(global_model, test_loader, device)\n",
    "        wb.log({\"epoch\": epoch, \"time\": time.time(), \"acc\": global_test_output[\"accuracy\"], \"loss\": global_test_output[\"test_loss\"]})\n",
    "        log.jsoninfo(global_test_output, \"Global Test Outut after Epoch {} of {}\".format(epoch, fedargs.epochs))\n",
    "        \n",
    "        # Update client models\n",
    "        for client in clients:\n",
    "            client_details[client]['model'] = copy.deepcopy(global_model)\n",
    "\n",
    "    # Clients\n",
    "    tasks = [process(client, epoch, client_details[client]['model'],\n",
    "                     client_details[client]['train_loader'],\n",
    "                     fedargs, device) for client in clients]\n",
    "    try:\n",
    "        updates = fedargs.loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    except KeyboardInterrupt as e:\n",
    "        log.error(\"Caught keyboard interrupt. Canceling tasks...\")\n",
    "        tasks.cancel()\n",
    "        fedargs.loop.run_forever()\n",
    "        tasks.exception()\n",
    "        \n",
    "    for client, update in zip(clients, updates):            \n",
    "        client_details[client]['model_update'] = update\n",
    "\n",
    "        if fedargs.enc:\n",
    "            enc_update = he.enc_model_update(update)\n",
    "            client_details[client]['model_update'] = enc_update\n",
    "    \n",
    "    '''\n",
    "    if fedargs.enc:            \n",
    "        # Parallel Enc\n",
    "        tasks = [enc(client_details[client]['model_update']) for client in client_details]\n",
    "        try:\n",
    "            updates = fedargs.loop.run_until_complete(asyncio.gather(*tasks))\n",
    "        except KeyboardInterrupt as e:\n",
    "            log.error(\"Caught keyboard interrupt. Canceling tasks...\")\n",
    "            tasks.cancel()\n",
    "            fedargs.loop.run_forever()\n",
    "            tasks.exception()\n",
    "            \n",
    "        for client, update in zip(clients, updates):            \n",
    "            client_details[client]['model_update'] = update\n",
    "    '''\n",
    "\n",
    "    client_model_updates = {client: details[\"model_update\"] for client, details in client_details.items()}\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2d6a9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<h1> End </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "\n",
    "for index, i in enumerate(a):\n",
    "    a[index] += 1\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fccaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:syft]",
   "language": "python",
   "name": "conda-env-syft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
